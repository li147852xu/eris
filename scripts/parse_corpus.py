"""è¯­æ–™è§£æè„šæœ¬ - æå–å…³é”®ä¿¡æ¯"""
import re
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional
from loguru import logger
from tqdm import tqdm
import sys

sys.path.append(str(Path(__file__).parent.parent))
from config import DATA_DIR, PROCESSED_DATA_DIR, LOG_DIR

# é…ç½®æ—¥å¿—
logger.add(LOG_DIR / "parse_corpus.log", rotation="10 MB")


class CorpusParser:
    """è¯­æ–™è§£æå™¨"""
    
    def __init__(self, data_dir: Path = DATA_DIR):
        self.data_dir = data_dir
        
    def parse_date_from_filename(self, filename: str) -> Optional[str]:
        """ä»æ–‡ä»¶åæå–æ—¥æœŸ"""
        match = re.search(r'(\d{4})-(\d{1,2})-(\d{1,2})', filename)
        if match:
            year, month, day = match.groups()
            return f"{year}-{int(month):02d}-{int(day):02d}"
        return None
    
    def split_sections(self, content: str) -> Dict[str, str]:
        """åˆ†å‰²æ—©è‡ªä¹ ã€ä¸»1ã€ä¸»2"""
        sections = {
            "æ—©è‡ªä¹ ": "",
            "ä¸»1": "",
            "ä¸»2": ""
        }
        
        # æŒ‰ä¸€çº§æ ‡é¢˜åˆ†å‰²
        parts = re.split(r'\n# ', content)
        
        for part in parts:
            if not part.strip():
                continue
            part = '# ' + part if not part.startswith('#') else part
            
            # åˆ¤æ–­æ˜¯å“ªä¸ªéƒ¨åˆ†
            if 'æ—©è‡ªä¹ ' in part[:50]:
                sections["æ—©è‡ªä¹ "] = part
            elif 'ï¼ˆä¸»1ï¼‰' in part[:100] or '(ä¸»1)' in part[:100]:
                sections["ä¸»1"] = part
            elif 'ï¼ˆä¸»2ï¼‰' in part[:100] or '(ä¸»2)' in part[:100]:
                sections["ä¸»2"] = part
                
        return sections
    
    def extract_key_info(self, content: str, date: str) -> Dict:
        """æå–å…³é”®ä¿¡æ¯"""
        info = {
            "date": date,
            "indices": [],  # æŒ‡æ•°ç‚¹ä½
            "sectors": [],  # æ¿å—
            "stocks": [],  # ä¸ªè‚¡
            "predictions": [],  # é¢„æµ‹
            "fund_flow": [],  # èµ„é‡‘æµå‘
            "sentiments": [],  # æƒ…ç»ªåˆ¤æ–­
        }
        
        # æå–æŒ‡æ•°ç‚¹ä½
        index_patterns = [
            r'(\d{4})ç‚¹',
            r'ä¸Šè¯.*?(\d{4})',
            r'æ²ªæŒ‡.*?(\d{4})',
            r'æ·±æˆæŒ‡.*?(\d{4})',
            r'åˆ›ä¸šæ¿.*?(\d{4})',
        ]
        for pattern in index_patterns:
            matches = re.findall(pattern, content)
            info["indices"].extend(matches)
        
        # æå–æ¿å—ï¼ˆä¸­æ–‡è¯æ±‡+å¯èƒ½çš„ä¿®é¥°è¯ï¼‰
        sector_keywords = [
            'åŠå¯¼ä½“', 'èŠ¯ç‰‡', 'AI', 'äººå·¥æ™ºèƒ½', 'å†›å·¥', 'æ–°èƒ½æº', 'å…‰ä¼', 
            'å‚¨èƒ½', 'é”‚ç”µ', 'åŒ»è¯', 'åˆ›æ–°è¯', 'æ¶ˆè´¹', 'é›¶å”®', 'åˆ¸å•†',
            'ç¨€åœŸ', 'æœ‰è‰²', 'ç…¤ç‚­', 'èˆªå¤©', 'æœºå™¨äºº', 'ä¼ åª’', 'æ¸¸æˆ',
            'CPO', 'PCB', 'ç®—åŠ›', 'å¤§æ¨¡å‹', 'å­˜å‚¨', 'å…‰åˆ»èƒ¶', 'ç¦å»º',
            'æµ·å—', 'ä¸¤å²¸', 'èˆªæ¯', 'æµ·é˜²', 'å›ºæ€ç”µæ± ', 'ç”µæ± ', 'ç™½é…’',
            'çŸ­å‰§', 'å½±è§†', 'è·¨å¢ƒç”µå•†', 'å†°é›ª', 'é€ çº¸', 'æœ‰æœºç¡…',
        ]
        for keyword in sector_keywords:
            if keyword in content:
                info["sectors"].append(keyword)
        
        # æå–ä¸ªè‚¡ï¼ˆå…¬å¸ç®€ç§°æ¨¡å¼ï¼‰
        stock_patterns = [
            r'[ä¸œè¥¿å—åŒ—ä¸­][\u4e00-\u9fa5]{1,3}(?=[\sã€ï¼Œã€‚ï¼])',  # æ–¹ä½+å­—
            r'[\u4e00-\u9fa5]{2,4}(?=æ¶¨åœ|è·Œåœ|ä¸Šæ¶¨|ä¸‹è·Œ)',
        ]
        for pattern in stock_patterns:
            matches = re.findall(pattern, content)
            info["stocks"].extend(matches)
        
        # æå–é¢„æµ‹å…³é”®è¯
        prediction_keywords = ['çœ‹æ¶¨', 'çœ‹è·Œ', 'éœ‡è¡', 'åå¼¹', 'åˆ†åŒ–', 'æ‰¿å‹', 'ä¼ç¨³', 'å†²é«˜å›è½']
        for keyword in prediction_keywords:
            if keyword in content:
                info["predictions"].append(keyword)
        
        # æå–èµ„é‡‘æµå‘
        fund_flow_pattern = r'(å‡€æµå…¥|å‡€æµå‡º).*?(\d+\.?\d*)äº¿'
        matches = re.findall(fund_flow_pattern, content)
        info["fund_flow"] = [f"{m[0]}{m[1]}äº¿" for m in matches]
        
        # å»é‡
        for key in ['indices', 'sectors', 'stocks', 'predictions']:
            info[key] = list(set(info[key]))
        
        return info
    
    def parse_file(self, file_path: Path) -> Optional[Dict]:
        """è§£æå•ä¸ªæ–‡ä»¶"""
        try:
            logger.info(f"è§£ææ–‡ä»¶: {file_path.name}")
            
            # æå–æ—¥æœŸ
            date = self.parse_date_from_filename(file_path.name)
            if not date:
                logger.warning(f"æ— æ³•ä»æ–‡ä»¶åæå–æ—¥æœŸ: {file_path.name}")
                return None
            
            # è¯»å–å†…å®¹
            content = file_path.read_text(encoding='utf-8')
            
            # åˆ†å‰²ç« èŠ‚
            sections = self.split_sections(content)
            
            # æå–å…³é”®ä¿¡æ¯
            result = {
                "filename": file_path.name,
                "date": date,
                "sections": sections,
                "key_info": {
                    section_name: self.extract_key_info(section_content, date)
                    for section_name, section_content in sections.items()
                    if section_content
                }
            }
            
            return result
            
        except Exception as e:
            logger.error(f"è§£ææ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
            return None
    
    def parse_all(self) -> List[Dict]:
        """è§£ææ‰€æœ‰è¯­æ–™æ–‡ä»¶"""
        results = []
        
        # è·å–æ‰€æœ‰.mdæ–‡ä»¶ï¼ˆæ’é™¤ReadMe.mdï¼‰
        md_files = sorted([
            f for f in self.data_dir.glob("*.md")
            if f.name != "ReadMe.md"
        ])
        
        logger.info(f"æ‰¾åˆ° {len(md_files)} ä¸ªè¯­æ–™æ–‡ä»¶")
        
        # æ·»åŠ è¿›åº¦æ¡
        for file_path in tqdm(md_files, desc="ğŸ“– è§£æè¯­æ–™æ–‡ä»¶", 
                             unit="æ–‡ä»¶", colour="cyan"):
            result = self.parse_file(file_path)
            if result:
                results.append(result)
        
        return results
    
    def save_parsed_data(self, data: List[Dict], output_file: str = "parsed_corpus.json"):
        """ä¿å­˜è§£æç»“æœ"""
        output_path = PROCESSED_DATA_DIR / output_file
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        logger.info(f"è§£æç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def main():
    """ä¸»å‡½æ•°"""
    parser = CorpusParser()
    results = parser.parse_all()
    parser.save_parsed_data(results)
    
    logger.info(f"æˆåŠŸè§£æ {len(results)} ä¸ªæ–‡ä»¶")
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    total_sections = sum(len([s for s in r['sections'].values() if s]) for r in results)
    logger.info(f"æ€»å…±ç« èŠ‚æ•°: {total_sections}")


if __name__ == "__main__":
    main()

