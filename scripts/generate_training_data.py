"""è‡ªåŠ¨ç”Ÿæˆè®­ç»ƒæ•°æ®è„šæœ¬ - ä½¿ç”¨GPT/Claudeè¾…åŠ©ç”Ÿæˆ"""
import json
import sys
import re
from pathlib import Path
from typing import Dict, List
from loguru import logger
import time
from tqdm import tqdm

# OpenAIæ˜¯å¯é€‰çš„ï¼Œåªæœ‰åœ¨ä½¿ç”¨GPTç”Ÿæˆæ—¶æ‰éœ€è¦
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

sys.path.append(str(Path(__file__).parent.parent))
from config import (
    PROCESSED_DATA_DIR, RAW_DATA_DIR, TRAINING_DATA_DIR,
    OPENAI_API_KEY, OPENAI_BASE_URL, TRAINING_CONFIG, LOG_DIR
)

# é…ç½®æ—¥å¿—
logger.add(LOG_DIR / "generate_training.log", rotation="10 MB")


class TrainingDataGenerator:
    """è®­ç»ƒæ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self):
        if OPENAI_AVAILABLE and OPENAI_API_KEY:
            self.client = OpenAI(
                api_key=OPENAI_API_KEY,
                base_url=OPENAI_BASE_URL
            )
            self.model = TRAINING_CONFIG["model"]
            self.temperature = TRAINING_CONFIG["temperature"]
            self.max_tokens = TRAINING_CONFIG["max_tokens"]
        else:
            self.client = None
            logger.warning("OpenAIä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ç®€åŒ–æ–¹æ³•ç”Ÿæˆè®­ç»ƒæ•°æ®")
        
    def load_parsed_corpus(self) -> List[Dict]:
        """åŠ è½½è§£æåçš„è¯­æ–™"""
        file_path = PROCESSED_DATA_DIR / "parsed_corpus.json"
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def load_market_data(self, date: str) -> Dict:
        """åŠ è½½å¸‚åœºæ•°æ®"""
        file_path = RAW_DATA_DIR / f"market_data_{date}.json"
        if file_path.exists():
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}
    
    def generate_training_sample(self, corpus_item: Dict, market_data: Dict, section_name: str) -> List[Dict]:
        """ç”Ÿæˆå•ä¸ªè®­ç»ƒæ ·æœ¬"""
        date = corpus_item['date']
        section_content = corpus_item['sections'].get(section_name, '')
        
        if not section_content:
            return []
        
        # æ„å»ºpromptè®©GPTç”Ÿæˆè®­ç»ƒå¯¹
        prompt = self._build_generation_prompt(date, section_name, section_content, market_data)
        
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é‡‘èæ•°æ®æ ‡æ³¨ä¸“å®¶ï¼Œæ“…é•¿å°†å¸‚åœºæ•°æ®å’Œä¸“å®¶åˆ†æè½¬æ¢ä¸ºç»“æ„åŒ–çš„è®­ç»ƒæ•°æ®ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            result_text = response.choices[0].message.content.strip()
            
            # è°ƒè¯•ï¼šæ‰“å°APIè¿”å›å†…å®¹
            logger.debug(f"APIè¿”å›å†…å®¹å‰100å­—: {result_text[:100]}")
            
            # å°è¯•æå–JSONï¼ˆæœ‰æ—¶APIä¼šåœ¨ä»£ç å—é‡Œè¿”å›ï¼‰
            if '```json' in result_text:
                # æå–ä»£ç å—ä¸­çš„JSON
                json_match = re.search(r'```json\s*(.*?)\s*```', result_text, re.DOTALL)
                if json_match:
                    result_text = json_match.group(1)
            elif '```' in result_text:
                # æå–æ™®é€šä»£ç å—
                json_match = re.search(r'```\s*(.*?)\s*```', result_text, re.DOTALL)
                if json_match:
                    result_text = json_match.group(1)
            
            # è§£æè¿”å›çš„JSON
            training_samples = json.loads(result_text)
            
            if not isinstance(training_samples, list):
                logger.error(f"è¿”å›çš„ä¸æ˜¯æ•°ç»„: {type(training_samples)}")
                return []
            
            logger.info(f"âœ“ ç”Ÿæˆ {len(training_samples)} ä¸ªæ ·æœ¬ ({date} - {section_name})")
            return training_samples
            
        except json.JSONDecodeError as e:
            logger.error(f"JSONè§£æå¤±è´¥ ({date} - {section_name}): {e}")
            logger.error(f"è¿”å›å†…å®¹: {result_text[:500]}")
            return []
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¤±è´¥ ({date} - {section_name}): {e}")
            return []
    
    def _extract_market_context_from_content(self, content: str, date: str) -> str:
        """ä»è¯­æ–™å†…å®¹ä¸­æ™ºèƒ½æå–å¸‚åœºèƒŒæ™¯ä¿¡æ¯"""
        import re
        
        contexts = []
        
        # æå–æŒ‡æ•°ç‚¹ä½
        points = re.findall(r'(\d{4})ç‚¹', content[:500])
        if points:
            contexts.append(f"æŒ‡æ•°ç‚¹ä½çº¦{points[0]}ç‚¹")
        
        # æå–æ¶¨è·Œæè¿°
        if 'ä¸Šæ¶¨' in content[:300] or 'æ¶¨' in content[:300]:
            contexts.append("å¸‚åœºä¸Šæ¶¨")
        elif 'ä¸‹è·Œ' in content[:300] or 'è·Œ' in content[:300]:
            contexts.append("å¸‚åœºä¸‹è·Œ")
        elif 'éœ‡è¡' in content[:300]:
            contexts.append("å¸‚åœºéœ‡è¡")
        
        # æå–æˆäº¤é‡ä¿¡æ¯
        volume = re.findall(r'(\d+\.?\d*)ä¸‡äº¿', content[:500])
        if volume:
            contexts.append(f"æˆäº¤é¢{volume[0]}ä¸‡äº¿")
        
        # æå–èµ„é‡‘æµå‘
        if 'å‡€æµå…¥' in content[:500]:
            contexts.append("èµ„é‡‘å‡€æµå…¥")
        elif 'å‡€æµå‡º' in content[:500]:
            contexts.append("èµ„é‡‘å‡€æµå‡º")
        
        return "ã€‚".join(contexts) if contexts else f"{date}å¸‚åœºæ•°æ®"
    
    def _build_generation_prompt(self, date: str, section_name: str, content: str, market_data: Dict) -> str:
        """æ„å»ºç”Ÿæˆè®­ç»ƒæ•°æ®çš„prompt"""
        
        # ä¼˜å…ˆä½¿ç”¨çœŸå®å¸‚åœºæ•°æ®ï¼Œå¦åˆ™ä»è¯­æ–™æå–
        if market_data and market_data.get('indices'):
            market_info = self._format_market_data(market_data)
        else:
            market_info = self._extract_market_context_from_content(content, date)
        
        # æ ¹æ®ç« èŠ‚ç±»å‹è°ƒæ•´prompt
        section_prompts = {
            'æ—©è‡ªä¹ ': {
                'context': 'ç›˜å‰é¢„æµ‹',
                'questions': [
                    "ä»Šå¤©å¤§ç›˜èµ°åŠ¿æ€ä¹ˆçœ‹ï¼Ÿ",
                    "ä»Šå¤©åº”è¯¥å…³æ³¨å“ªäº›æ¿å—ï¼Ÿ",
                    "ä»Šå¤©æœ‰ä»€ä¹ˆæ“ä½œå»ºè®®ï¼Ÿ",
                    "ä»Šå¤©ç›˜å‰æ€ä¹ˆåˆ†æï¼Ÿ"
                ]
            },
            'ä¸»1': {
                'context': 'å½“æ—¥å¤ç›˜',
                'questions': [
                    "ä»Šå¤©å¸‚åœºè¡¨ç°å¦‚ä½•ï¼Ÿ",
                    "ä»Šå¤©èµ„é‡‘æµå‘å“ªé‡Œï¼Ÿ",
                    "ä»Šå¤©æœ‰ä»€ä¹ˆå€¼å¾—æ³¨æ„çš„ï¼Ÿ",
                    "ä»Šå¤©å¤§ç›˜èµ°åŠ¿æ€ä¹ˆæ ·ï¼Ÿ"
                ]
            },
            'ä¸»2': {
                'context': 'æ˜æ—¥é¢„æµ‹',
                'questions': [
                    "æ˜å¤©å¤§ç›˜æ€ä¹ˆçœ‹ï¼Ÿ",
                    "æ˜å¤©åº”è¯¥å…³æ³¨ä»€ä¹ˆï¼Ÿ",
                    "æ˜å¤©æœ‰ä»€ä¹ˆæ“ä½œå»ºè®®ï¼Ÿ",
                    "æ˜å¤©å¸‚åœºæ€ä¹ˆå¸ƒå±€ï¼Ÿ"
                ]
            }
        }
        
        section_info = section_prompts.get(section_name, section_prompts['ä¸»1'])
        
        prompt = f"""è¯·æ ¹æ®ä»¥ä¸‹é‡‘èåˆ†ææ–‡ç« ï¼Œç”Ÿæˆ4-5ä¸ªé«˜è´¨é‡çš„é—®ç­”è®­ç»ƒæ ·æœ¬ã€‚

**æ—¥æœŸ**: {date}
**æ–‡ç« ç±»å‹**: {section_name}ï¼ˆ{section_info['context']}ï¼‰
**å¸‚åœºèƒŒæ™¯**: {market_info}

**ä¸“å®¶åˆ†æåŸæ–‡**ï¼ˆèŠ‚é€‰ï¼‰:
{content[:2500]}

**ç”Ÿæˆè¦æ±‚**:

1. **é—®é¢˜è®¾è®¡**ï¼šæ¨¡æ‹ŸçœŸå®ç”¨æˆ·æé—®ï¼Œå¯å‚è€ƒï¼š
{chr(10).join(['   - ' + q for q in section_info['questions']])}

2. **å›ç­”è¦æ±‚**ï¼š
   - ä¸¥æ ¼åŸºäºåŸæ–‡å†…å®¹ï¼Œä¸è¦ç¼–é€ 
   - å¿…é¡»ä¿æŒåŸæ–‡ç‹¬ç‰¹é£æ ¼ï¼š
     * "è‰åŸ" æŒ‡ä»£è‚¡å¸‚
     * "ç¾Š" æŒ‡ä»£è‚¡ç¥¨  
     * "åƒæ¡ƒ" æŒ‡ä»£äºæŸ
     * "åšT" æŒ‡ä»£é«˜æŠ›ä½å¸
     * ä¿æŒå£è¯­åŒ–ã€ç›´ç™½çš„è¡¨è¾¾
   - ç»“åˆå¸‚åœºèƒŒæ™¯ç»™å‡ºå…·ä½“å»ºè®®
   - å›ç­”é•¿åº¦150-300å­—

3. **è¾“å‡ºæ ¼å¼**ï¼š
ä¸¥æ ¼è¿”å›JSONæ•°ç»„ï¼Œæ¯ä¸ªå¯¹è±¡åŒ…å«ï¼š
- instruction: ç”¨æˆ·é—®é¢˜
- input: å¸‚åœºèƒŒæ™¯ï¼ˆç®€çŸ­ï¼Œ1-2å¥è¯ï¼‰
- output: åŠ©æ‰‹å›ç­”ï¼ˆä¿æŒåŸæ–‡é£æ ¼ï¼Œå…·ä½“å¯æ“ä½œï¼‰
- section_type: "{section_name}"
- date: "{date}"

**åªè¿”å›JSONæ•°ç»„ï¼Œä¸è¦æœ‰ä»»ä½•å…¶ä»–æ–‡å­—æˆ–è§£é‡Šã€‚**
"""
        return prompt
    
    def _format_market_data(self, market_data: Dict) -> str:
        """æ ¼å¼åŒ–å¸‚åœºæ•°æ®ä¸ºæ–‡æœ¬"""
        if not market_data:
            return "æš‚æ— å¸‚åœºæ•°æ®"
        
        lines = []
        
        # æŒ‡æ•°æ•°æ®
        if 'indices' in market_data and market_data['indices']:
            lines.append("**æŒ‡æ•°æ•°æ®**:")
            for index_name, index_data in market_data['indices'].items():
                lines.append(f"- {index_name}: æ”¶ç›˜{index_data.get('close', 'N/A')}, "
                           f"æ¶¨è·Œå¹…{index_data.get('change_pct', 'N/A')}%")
        
        # å¸‚åœºæ¦‚å†µ
        if 'market_overview' in market_data:
            overview = market_data['market_overview']
            if 'market_stats' in overview:
                stats = overview['market_stats']
                lines.append(f"\n**å¸‚åœºç»Ÿè®¡**: ä¸Šæ¶¨{stats.get('up_count', 'N/A')}å®¶, "
                           f"ä¸‹è·Œ{stats.get('down_count', 'N/A')}å®¶")
            if 'turnover' in overview:
                turnover = overview['turnover']
                lines.append(f"**æˆäº¤é¢**: {turnover.get('amount', 'N/A')}{turnover.get('unit', '')}")
        
        # èµ„é‡‘æµå‘
        if 'fund_flow' in market_data and market_data['fund_flow']:
            lines.append("\n**èµ„é‡‘æµå‘**:")
            for fund_type, fund_data in market_data['fund_flow'].items():
                lines.append(f"- {fund_type}: å‡€æµå…¥{fund_data.get('net_inflow', 'N/A')}{fund_data.get('unit', '')}")
        
        return '\n'.join(lines) if lines else "æš‚æ— è¯¦ç»†å¸‚åœºæ•°æ®"
    
    def generate_all_training_data(self) -> List[Dict]:
        """ç”Ÿæˆæ‰€æœ‰è®­ç»ƒæ•°æ®"""
        logger.info("å¼€å§‹ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆä½¿ç”¨DeepSeek APIï¼‰")
        
        # åŠ è½½è§£æåçš„è¯­æ–™
        corpus_data = self.load_parsed_corpus()
        logger.info(f"åŠ è½½äº† {len(corpus_data)} ä¸ªè¯­æ–™æ–‡ä»¶")
        
        all_training_samples = []
        total_sections = len(corpus_data) * 3  # æ¯å¤©3ä¸ªç« èŠ‚
        
        try:
            # åˆ›å»ºæ€»è¿›åº¦æ¡
            with tqdm(total=total_sections, desc="ğŸ¤– DeepSeekç”Ÿæˆè®­ç»ƒæ•°æ®", 
                      unit="ç« èŠ‚", colour="green") as pbar:
                
                for idx, corpus_item in enumerate(corpus_data):
                    date = corpus_item['date']
                    
                    # å°è¯•åŠ è½½å¯¹åº”æ—¥æœŸçš„å¸‚åœºæ•°æ®ï¼ˆæš‚æ—¶è·³è¿‡ï¼Œä½¿ç”¨è¯­æ–™ä¿¡æ¯ï¼‰
                    market_data = self.load_market_data(date)
                    # if not market_data:
                    #     logger.debug(f"{date} æ²¡æœ‰å¸‚åœºæ•°æ®ï¼Œå°†ä½¿ç”¨è¯­æ–™ä¸­çš„ä¿¡æ¯")
                    
                    # ä¸ºæ¯ä¸ªç« èŠ‚ç”Ÿæˆè®­ç»ƒæ ·æœ¬
                    for section_name in ['æ—©è‡ªä¹ ', 'ä¸»1', 'ä¸»2']:
                        pbar.set_description(f"ğŸ¤– å¤„ç† {date} - {section_name}")
                        
                        samples = self.generate_training_sample(corpus_item, market_data, section_name)
                        all_training_samples.extend(samples)
                        
                        pbar.update(1)
                        pbar.set_postfix({"å·²ç”Ÿæˆæ ·æœ¬": len(all_training_samples)})
                        
                        # å»¶è¿Ÿé¿å…APIé™æµ
                        time.sleep(0.5)
                    
                    # æ¯å¤„ç†5å¤©ä¿å­˜ä¸€æ¬¡ï¼ˆé˜²æ­¢ä¸­æ–­ä¸¢å¤±ï¼‰
                    if (idx + 1) % 5 == 0:
                        self.save_training_data(all_training_samples, "training_dataset_backup.json")
                        logger.info(f"ğŸ’¾ å·²ä¿å­˜ä¸­é—´ç»“æœ ({idx+1}/{len(corpus_data)} å¤©)")
        
        except KeyboardInterrupt:
            logger.warning("âš ï¸ æ£€æµ‹åˆ°ä¸­æ–­ï¼Œä¿å­˜å·²ç”Ÿæˆçš„æ•°æ®...")
            if all_training_samples:
                self.save_training_data(all_training_samples, "training_dataset_interrupted.json")
                logger.info(f"å·²ä¿å­˜ {len(all_training_samples)} ä¸ªæ ·æœ¬åˆ° training_dataset_interrupted.json")
            raise
        
        logger.info(f"âœ… å…±ç”Ÿæˆ {len(all_training_samples)} ä¸ªè®­ç»ƒæ ·æœ¬")
        return all_training_samples
    
    def save_training_data(self, data: List[Dict], filename: str = "training_dataset.json"):
        """ä¿å­˜è®­ç»ƒæ•°æ®"""
        output_path = TRAINING_DATA_DIR / filename
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
        
        # åŒæ—¶ä¿å­˜ä¸ºjsonlæ ¼å¼ï¼ˆå¾®è°ƒå¸¸ç”¨æ ¼å¼ï¼‰
        jsonl_path = TRAINING_DATA_DIR / filename.replace('.json', '.jsonl')
        with open(jsonl_path, 'w', encoding='utf-8') as f:
            for item in data:
                f.write(json.dumps(item, ensure_ascii=False) + '\n')
        
        logger.info(f"JSONLæ ¼å¼å·²ä¿å­˜åˆ°: {jsonl_path}")
    
    def generate_simple_training_data(self) -> List[Dict]:
        """ç”Ÿæˆç®€åŒ–ç‰ˆè®­ç»ƒæ•°æ®ï¼ˆä¸ä¾èµ–GPTï¼Œç›´æ¥ä»è¯­æ–™ç”Ÿæˆï¼‰"""
        logger.info("ç”Ÿæˆç®€åŒ–ç‰ˆè®­ç»ƒæ•°æ®ï¼ˆä¸ä½¿ç”¨APIï¼‰")
        
        corpus_data = self.load_parsed_corpus()
        all_samples = []
        
        # æ·»åŠ è¿›åº¦æ¡
        for corpus_item in tqdm(corpus_data, desc="ğŸ“ ç”Ÿæˆç®€åŒ–è®­ç»ƒæ•°æ®", 
                                unit="æ–‡ä»¶", colour="blue"):
            date = corpus_item['date']
            sections = corpus_item['sections']
            
            # æ—©è‡ªä¹  -> é¢„æµ‹ç±»é—®é¢˜
            if sections.get('æ—©è‡ªä¹ '):
                sample = {
                    "instruction": f"è¯·åˆ†æ{date}ä»Šå¤©çš„å¸‚åœºèµ°åŠ¿ï¼Œåº”è¯¥å…³æ³¨å“ªäº›æ–¹å‘ï¼Ÿ",
                    "input": f"æ—¥æœŸï¼š{date}",
                    "output": sections['æ—©è‡ªä¹ '][:500],  # å–å‰500å­—ä½œä¸ºç¤ºä¾‹
                    "section_type": "æ—©è‡ªä¹ ",
                    "date": date
                }
                all_samples.append(sample)
            
            # ä¸»1 -> å¤ç›˜ç±»é—®é¢˜
            if sections.get('ä¸»1'):
                sample = {
                    "instruction": f"è¯·å¤ç›˜{date}ä»Šå¤©çš„å¸‚åœºè¡¨ç°",
                    "input": f"æ—¥æœŸï¼š{date}",
                    "output": sections['ä¸»1'][:500],
                    "section_type": "ä¸»1",
                    "date": date
                }
                all_samples.append(sample)
            
            # ä¸»2 -> æ˜æ—¥é¢„æµ‹é—®é¢˜
            if sections.get('ä¸»2'):
                sample = {
                    "instruction": f"{date}æ”¶ç›˜åï¼Œæ˜å¤©åº”è¯¥å¦‚ä½•å¸ƒå±€ï¼Ÿ",
                    "input": f"æ—¥æœŸï¼š{date}",
                    "output": sections['ä¸»2'][:500],
                    "section_type": "ä¸»2",
                    "date": date
                }
                all_samples.append(sample)
        
        logger.info(f"âœ… ç”Ÿæˆäº† {len(all_samples)} ä¸ªç®€åŒ–è®­ç»ƒæ ·æœ¬")
        return all_samples


def main():
    """ä¸»å‡½æ•°"""
    import sys
    generator = TrainingDataGenerator()
    
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    use_gpt = '--use-gpt' in sys.argv or '-g' in sys.argv
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šå‚æ•°ï¼Œé»˜è®¤ä½¿ç”¨GPTï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if not use_gpt and OPENAI_AVAILABLE and OPENAI_API_KEY:
        use_gpt = True
        logger.info("æ£€æµ‹åˆ°APIé…ç½®ï¼Œè‡ªåŠ¨å¯ç”¨GPTå¢å¼ºæ¨¡å¼")
    
    if use_gpt and OPENAI_AVAILABLE and OPENAI_API_KEY:
        logger.info("ä½¿ç”¨DeepSeek APIç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®")
        training_data = generator.generate_all_training_data()
    else:
        logger.info("ä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼ˆä¸è°ƒç”¨APIï¼‰")
        training_data = generator.generate_simple_training_data()
    
    generator.save_training_data(training_data)
    
    logger.info("è®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆï¼")


if __name__ == "__main__":
    main()

